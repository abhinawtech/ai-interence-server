# ================================================================================================
# AI INFERENCE SERVER - ENVIRONMENT CONFIGURATION
# ================================================================================================
#
# This file contains all configurable environment variables for the AI inference server
# Copy this file to .env and customize the values for your deployment environment
#
# ================================================================================================

# ================================================================================================
# SERVER CONFIGURATION
# ================================================================================================

# Server port (default: 3000)
PORT=3000

# Environment type (development, staging, production, testing)
ENVIRONMENT=development

# Thread configuration for compute-intensive tasks
RAYON_NUM_THREADS=4

# Disable tokenizer parallelism (recommended for Tokio compatibility)
TOKENIZERS_PARALLELISM=false

# ================================================================================================
# BATCH PROCESSING CONFIGURATION
# ================================================================================================

# Maximum batch size for request aggregation (default: 4)
BATCH_MAX_SIZE=4

# Maximum wait time in milliseconds before processing batch (default: 100)
BATCH_MAX_WAIT_MS=100

# Maximum queue size to prevent memory exhaustion (default: 50)
BATCH_MAX_QUEUE_SIZE=50

# ================================================================================================
# QDRANT VECTOR DATABASE CONFIGURATION
# ================================================================================================

# Qdrant server URL
QDRANT_URL=http://localhost:6333

# Qdrant API key (optional, for authentication)
# QDRANT_API_KEY=your-api-key-here

# Vector database environment configuration
VECTOR_ENVIRONMENT=development

# Connection pool settings (environment-specific defaults applied)
# QDRANT_MIN_CONNECTIONS=2
# QDRANT_MAX_CONNECTIONS=10
# QDRANT_CONNECTION_TIMEOUT_MS=30000
# QDRANT_IDLE_TIMEOUT_MS=300000

# Retry configuration
# QDRANT_MAX_RETRY_ATTEMPTS=3
# QDRANT_INITIAL_RETRY_DELAY_MS=1000
# QDRANT_MAX_RETRY_DELAY_MS=30000

# Health check configuration
# QDRANT_HEALTH_CHECK_INTERVAL_MS=60000
# QDRANT_HEALTH_CHECK_TIMEOUT_MS=5000

# ================================================================================================
# EMBEDDING GENERATION CONFIGURATION
# ================================================================================================

# Embedding vector dimension (default: 384)
EMBEDDING_DIMENSION=384

# Whether to normalize embeddings to unit length (default: true)
EMBEDDING_NORMALIZE=true

# Embedding generation strategy (tfidf, sentence, char_ngram, word_level)
EMBEDDING_STRATEGY=sentence

# Maximum text length to process for embeddings (default: 2048)
EMBEDDING_MAX_TEXT_LENGTH=2048

# ================================================================================================
# COLLECTION CONFIGURATION
# ================================================================================================

# Default collection names
DEFAULT_EMBEDDINGS_COLLECTION=ai_embeddings
DEFAULT_CONVERSATIONS_COLLECTION=conversations

# HNSW index configuration for AI workloads
# HNSW_M=64
# HNSW_EF_CONSTRUCT=1024
# HNSW_FULL_SCAN_THRESHOLD=20000

# ================================================================================================
# MONITORING AND OBSERVABILITY
# ================================================================================================

# Enable detailed logging (debug, info, warn, error)
RUST_LOG=info

# Prometheus metrics endpoint (if enabled)
# METRICS_ENABLED=true
# METRICS_PORT=9090

# Health check configuration
# HEALTH_CHECK_ENABLED=true
# HEALTH_CHECK_INTERVAL_MS=30000

# ================================================================================================
# SECURITY CONFIGURATION
# ================================================================================================

# API authentication (if implemented)
# API_KEY_REQUIRED=false
# API_KEYS=key1,key2,key3

# Rate limiting configuration
# RATE_LIMIT_ENABLED=false
# RATE_LIMIT_REQUESTS_PER_MINUTE=100
# RATE_LIMIT_BURST_SIZE=10

# CORS configuration
# CORS_ENABLED=true
# CORS_ALLOWED_ORIGINS=*
# CORS_ALLOWED_METHODS=GET,POST,PUT,DELETE,OPTIONS

# ================================================================================================
# MODEL CONFIGURATION
# ================================================================================================

# Model loading configuration
# MODEL_CACHE_DIR=./models
# MODEL_MAX_MEMORY_MB=4096

# Model version management
# MAX_MODEL_VERSIONS=3
# MODEL_HEALTH_CHECK_INTERVAL_MS=60000

# ================================================================================================
# DEPLOYMENT SPECIFIC SETTINGS
# ================================================================================================

# Docker deployment settings
# DOCKER_NETWORK=ai-network
# CONTAINER_NAME=ai-inference-server

# Kubernetes deployment settings
# K8S_NAMESPACE=ai-inference-system
# K8S_SERVICE_ACCOUNT=ai-inference-sa

# ================================================================================================
# EXAMPLE CONFIGURATIONS BY ENVIRONMENT
# ================================================================================================

# DEVELOPMENT ENVIRONMENT
# ENVIRONMENT=development
# QDRANT_URL=http://localhost:6333
# BATCH_MAX_SIZE=2
# EMBEDDING_DIMENSION=256
# RUST_LOG=debug

# STAGING ENVIRONMENT  
# ENVIRONMENT=staging
# QDRANT_URL=http://qdrant-staging:6333
# BATCH_MAX_SIZE=4
# EMBEDDING_DIMENSION=384
# RUST_LOG=info

# PRODUCTION ENVIRONMENT
# ENVIRONMENT=production
# QDRANT_URL=http://qdrant-cluster:6333
# QDRANT_API_KEY=your-production-api-key
# BATCH_MAX_SIZE=8
# EMBEDDING_DIMENSION=768
# RUST_LOG=warn
# RATE_LIMIT_ENABLED=true
# API_KEY_REQUIRED=true

